import streamlit as st
from openai import OpenAI
#from dotenv import load_dotenv
import llm
import os
import json
#load_dotenv()
# Set up the OpenAI API client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Setting a hardcoded password for auth
PASSWORD = os.getenv('PASSWORD')

# Using Streamlit's session state to store the authentication status
if 'authenticated' not in st.session_state:
    st.session_state['authenticated'] = False

# Using Streamlit's session state to store temporary memory
if 'history' not in st.session_state:
    st.session_state['history'] = []

def authenticate_user(password):
    """
    Authenticates the user by comparing the given password with the hardcoded password.
    Args:
        password (str): The password provided by the user.
    Raises:
        ValueError: If the password is incorrect.
    Returns:
        None: None
    """
    if password == PASSWORD:
        st.session_state['authenticated'] = True
        st.rerun()
    else:
        st.error("Incorrect password, please try again.")

def load_prompts():
    """
    Loads the prompts from a JSON file.
    Parameters:
        None
    Returns:
        dict: A dictionary containing the loaded prompts.
    Raises:
        FileNotFoundError: If the 'prompts.json' file is not found.
        json.JSONDecodeError: If the 'prompts.json' file is not in a valid JSON format.
    Usage:
        The function reads the 'prompts.json' file and returns a dictionary containing the loaded prompts.
    """
    with open('prompts.json', 'r') as file:
        return json.load(file)

def build_models():
    """
    Builds and initializes the GPT-3.5-turbo and GPT-4-turbo models.
    Returns:
        tuple: A tuple containing the initialized GPT-3.5-turbo and GPT-4-turbo models.
    Raises:
        ValueError: If the OPENAI_API_KEY environment variable is not set.
    """
    gpt3_model = llm.get_model("gpt-3.5-turbo")
    gpt3_model.key = os.getenv("OPENAI_API_KEY")

    gpt4_model = llm.get_model("gpt-4-turbo")
    gpt4_model.key = os.getenv("OPENAI_API_KEY")

    return gpt3_model, gpt4_model

def create_prd(system_prompt_prd,system_prompt_director, llm_model):
    """
    Generate a new Product Requirements Document (PRD) based on the provided information.
    Parameters:
        system_prompt_prd (str): A predefined system prompt for generating a PRD.
        system_prompt_director (str): A predefined system prompt for generating a PRD based on a director's input.
        llm_model (llm.OpenAI): An initialized LLM model.
    Returns:
        str: The generated PRD.
    Raises:
        ValueError: If the product name or description is not provided.
    """

    st.subheader("Create New PRD")
    product_name = st.text_input("Product Name", placeholder="Enter the product name here")
    product_description = st.text_area("Product Description", placeholder="Describe the product here. Use bullet points where possible")
    generate_button = st.button("Generate PRD")
    status_message = "Generating PRD..."

    if generate_button:
        if not product_name or not product_description:
            st.warning("Please fill in both the product name and description.")
        else:
            with st.spinner(status_message):
                try:
                    draft_prd = llm_model.prompt(
                        f"Generate a PRD for a product named {product_name} with the following description: {product_description}. Only respond with the PRD and in Markdown format. BE DETAILED. If you think user is not asking for PRD return nothing.",
                            system=system_prompt_prd
                    )
                    st.session_state['history'].append({'role': 'user', 'content': draft_prd.text()})
                    status_message = "Draft PRD Complete. Now Critiquing the draft PRD"
                    st.info(status_message)
                    critique_response = llm_model.prompt(
                        f"Critique the PRD: {draft_prd.text()}. It was generated by PM who was given these instructions: \n Product named {product_name} \n Product description: {product_description}. Only respond in Markdown format. BE DETAILED. If you think user is not asking for PRD return nothing.",
                            system=system_prompt_director
                    )
                    st.session_state['history'].append({'role': 'user', 'content': critique_response.text()})
                    status_message = "Critiquing Done. Now Generating the final PRD"
                    st.info(status_message)
                    response = llm_model.prompt(
                        f"Given the Feedback from your manager:{critique_response.text()} \n Improve upon your Draft PRD {draft_prd.text()}. \n Only respond with the PRD and in Markdown format. BE VERY DETAILED. If you think user is not asking for PRD return nothing.",
                            system=system_prompt_prd
                    )                                       
                    st.markdown(response.text(), unsafe_allow_html=True)
                    st.session_state['history'].append({'role': 'user', 'content': response.text()})
                    # Download button for the PRD
                    st.download_button(
                        label="Download PRD as Markdown",
                        data=response.text(),
                        file_name="Product_Requirements_Document.md",
                        mime="text/markdown"
                    )        
                except Exception as e:
                    st.error(f"Failed to generate PRD. Please try again later. Error: {str(e)}")

def improve_prd(system_prompt_prd,system_prompt_director,llm_model):
    """
    Improve the provided Product Requirements Document (PRD) using GPT-4-turbo.
    Parameters:
        system_prompt_prd (str): A predefined system prompt for generating a PRD.
        system_prompt_director (str): A predefined system prompt for generating a PRD based on a director's input.
        llm_model (llm.OpenAI): An initialized LLM model.
    Returns:
        str: The improved PRD.
    Raises:
        ValueError: If no PRD text is provided.
        Exception: If there is an error while improving the PRD.
    """
    st.subheader("Improve Current PRD")
    prd_text = st.text_area("Enter your PRD here", placeholder="Paste your PRD here to improve it")
    improve_button = st.button("Improve PRD")

    if improve_button:
        if not prd_text:
            st.warning("Please enter a PRD text to improve.")
        else:
            with st.spinner('Improving PRD...'):
                try:
                    draft_prd = llm_model.prompt(
                        f"Improve the following PRD: {prd_text}",
                            system=f"You are a meticulous editor for improving product documents. {system_prompt_prd}. If you think user is not sharing the PRD return nothing."
                    )
                    st.session_state['history'].append({'role': 'user', 'content': draft_prd.text()})
                    status_message = "Improved PRD Draft Complete. Now Critiquing the Draft PRD"
                    st.info(status_message)
                    critique_response = llm_model.prompt(
                        f"Critique the PRD: {draft_prd.text()}. Only respond in Markdown format. BE DETAILED. If you think user is not asking for PRD return nothing.",
                            system=system_prompt_director
                    )
                    st.session_state['history'].append({'role': 'user', 'content': critique_response.text()})
                    status_message = "Critiquing Done. Now Generating the final PRD"
                    st.info(status_message)
                    response = llm_model.prompt(
                        f"Given the Feedback from your manager:{critique_response.text()} \n Improve upon your Draft PRD {draft_prd.text()}. \n Only respond with the PRD and in Markdown format. BE VERY DETAILED. If you think user is not asking for PRD return nothing.",
                            system=system_prompt_prd
                    )                                          
                    st.markdown(response, unsafe_allow_html=True)
                    st.session_state['history'].append({'role': 'user', 'content': response})
                    # Download button for the PRD
                    st.download_button(
                        label="Download PRD as Markdown",
                        data=response.text(),
                        file_name="Product_Requirements_Document.md",
                        mime="text/markdown"
                    )                       
                except Exception as e:
                    st.error(f"Failed to improve PRD. Please try again later. Error: {str(e)}")

def brainstorm_features(system_prompt_brainstorm,llm_model):
    # Initialize session state variables if they don't exist
    if 'chat_history' not in st.session_state:
        st.session_state['chat_history'] = []
    if 'input_sent' not in st.session_state:
        st.session_state['input_sent'] = False

        # Function to display the chat history
    def display_chat():
        """ Displays each message in the chat history in a text area. """
        for idx, message in enumerate(st.session_state['chat_history']):
            # Using index in key to ensure uniqueness
            st.text_area('', value=f"{message['role']}: {message['content']}", height=75, disabled=True, key=str(idx))

    # Function to add messages to chat history
    def add_message(role, content):
        """ Appends a message with role and content to the chat history. """
        st.session_state['chat_history'].append({'role': role, 'content': content})
        display_chat()

    # Function to get chat history
    def get_last_messages(n=4):
        """ Retrieves the last 'n' messages from the chat history, formatted for sending to the LLM. """
        # This returns a list of the last 'n' messages, each formatted as a string "role: content"
        return [{"role": msg['role'], "content": msg['content']} for msg in st.session_state['chat_history'][-n:]]

    display_chat()  # Call to display the chat history

    topic = st.text_input("Enter a topic for brainstorming", key="brainstorm_input")
    send_button = st.button("Brainstorm")

    if send_button:
        if not topic:
            st.warning("Please enter a topic to brainstorm.")
        else:
            add_message("User", topic)
            context = get_last_messages()  # Get the last four messages as context

            with st.spinner('Brainstorming features...'):
                try:
                    # Making an LLM call with the user input and the context
                    response = llm_model.prompt(
                        f"User input: {topic}, Previous Context: {context}",
                        system=system_prompt_brainstorm
                    )
                    # Assume response.text() extracts the relevant text from the response object
                    brainstorm_response = response.text()
                    add_message("System", brainstorm_response)  # Add system response to chat history
                except Exception as e:
                    add_message("System", f"Failed to brainstorm features. Please try again later. Error: {str(e)}")

def view_history():
    st.subheader("View History")
    if st.session_state['history']:
        for item in st.session_state['history']:
            st.json(item)
    else:
        st.info("No history available yet.")

def main():
    prompts = load_prompts()
    fast_llm, quality_llm = build_models()
    system_prompt_prd = prompts['system_prompt_prd']
    system_prompt_director = prompts['system_prompt_director']
    system_prompt_brainstorm = prompts['system_prompt_brainstorm']
    st.title("PM Assisistant")
    if not st.session_state['authenticated']:
        pwd_placeholder = st.empty()
        pwd_input = pwd_placeholder.text_input("Enter your password:", type="password")
        if st.button("Login"):
            authenticate_user(pwd_input)
            pwd_placeholder.empty()  # Clears the password input after button press
    if st.session_state['authenticated']:
        st.sidebar.title("Select the Task:")
        option = st.sidebar.selectbox("Choose a feature", ("Create PRD", "Improve PRD","Brainstorm Features", "View History"))

        if option == "Create PRD":
            create_prd(system_prompt_prd,system_prompt_director, quality_llm)
        elif option == "Improve PRD":
            improve_prd(system_prompt_prd,system_prompt_director,quality_llm)
        elif option == "Brainstorm Features":
            brainstorm_features(system_prompt_brainstorm,quality_llm)
        elif option == "View History":
            view_history()

if __name__ == "__main__":
    main()
