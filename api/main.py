from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Optional, List
import uvicorn
from llm.openai_llm import OpenAIWrapper
from llm.groq_llm import GroqWrapper
from dotenv import load_dotenv
import os

load_dotenv()
app = FastAPI()

class GenerateTextRequest(BaseModel):
    provider: str = Field(..., description="The text generation service provider, e.g., 'groq' or 'openai'.")
    model: str = Field(..., description="The model identifier, specifying which language model to use for text generation. gpt4, llama3-8b-8192, llama3-70b-8192")
    prompt: str = Field(..., description="The input prompt to the language model based on which the text is generated.")
    temperature: float = Field(0.5, description="Controls the randomness of the output. Lower values make it more deterministic.")
    return_prompt: bool = Field(False, description="A boolean flag to specify whether to return the original prompt with the generated text.")
    max_tokens: Optional[int] = Field(4000, description="The maximum number of tokens to generate. Default is 4000.")
    system_instructions: str = Field("You are working for PropertyGuru", description="Instructions that define the context or constraints under which the model operates. Typically used to create agents or give personality.")


class GenerateTextResponse(BaseModel):
    generated_text: str = Field(..., description="The text generated by the AI model based on the input prompt.")
    input_token: int = Field(0, description="The number of tokens in the input prompt. Defaults to 0 if not provided.")
    output_token: int = Field(0, description="The number of tokens generated by the AI model as output.")
    prompt_returned: Optional[str] = Field(None, description="The original prompt returned along with the output text, if requested.")

@app.post("/generate-text", response_model=GenerateTextResponse)
async def generate_text(request: GenerateTextRequest):
    try:
        if request.provider == "openai":
            client = OpenAIWrapper(model=request.model, system_prompt=request.system_instructions)
            generated_text, input_tokens, output_tokens = client.generate_text(
                prompt=request.prompt,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
            )
        else:
            client = GroqWrapper(model=request.model,system_prompt=request.system_instructions)
            generated_text, input_tokens, output_tokens = client.generate_text(
                prompt=request.prompt,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
            )
        result = GenerateTextResponse(
            generated_text=generated_text,
            input_token=input_tokens,
            output_token=output_tokens
        )
        if request.return_prompt:
            result.prompt_returned = request.prompt

        return result
    except HTTPException as e:
        raise e
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8024)
